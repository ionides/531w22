---
title: "Atmospheric CO2 Level Trends and Seasonality [1958-2022]"
subtitle: "STATS 531 Midterm Project"
date: "2/20/2022"
output: html_document
---

```{r libraries, include=FALSE, message=FALSE}
require(ggplot2)
require(pander)
require(knitr)
require(readr)
require(dplyr)
require("readxl")
require(tidyverse)
require(knitr)
require("astsa")
require("forecast")
require(aTSA)
```

## Introduction
Understanding the impact of atmospheric concentration of carbon dioxide is of great importance in the ongoing effects to address global climate change.  Carbon dioxide (CO~2~) is generated by human activity (e.g. burning of fossil fuels) and natural processes (e.g. volcanic eruptions) and its level impacts the amount of heat that is "trapped" in the atmosphere.~1~    CO~2~ levels have increased 49% since the beginning of the industrial revolution~1~ and are higher than they have been at any point of the last 400,000 years.~2~ Research by the National Oceanic and Atmospheric Administration (NOAA) shows a very clear link between temperature change and atmospheric CO~2~ level over time.~3~  When the levels of CO~2~ continue to climb, so does the global temperature change. This is supported by concurrent research that shows an corresponding and continual rise in oceanic surface temperature.~4~

Through this investigation we seek to corroborate the reported trend of atmospheric CO~2~ level, understand the underlying structure such as seasonality, and use modeling in both the time and frequency domains to quantify the associated trends and cycles of atmospheric CO~2~ levels.  This information will provide scientists and policy makers critical information about which policies, and the timing by which those policies, can be most impactful. 

\

## Data

Atmospheric CO~2~ levels are measured each hour at the NASA Mauna Loa Observatory (Hawaii).  This data is publicly available via the NOAA Global Monitoring Laboratory and by FTP transfer from the [NASA Global Climate Change Website](https://climate.nasa.gov/vital-signs/carbon-dioxide/).~1,5~  Atmospheric CO~2~ level data through 2020 from the Mauna Loa Observatory (as of the writing of this report) were also available through the [NOAA Global Monitoring Laboratory Data Finder](https://gml.noaa.gov/dv/data/index.php?category=Greenhouse%2BGases&parameter_name=Carbon%2BDioxide&site=MLO).~6~


Our variables of interest from the NOAA CO~2~ data set include Year, Month, Date (decimal), Monthly Average, and Deseasonalized Monthly Average. The NOAA dataset also includes variables for Number of Days, Standard Deviation of Number of Days, and Uncertainty Estimate of Monthly Average which are maintained and described in the linked dataset but are not utilized in this analysis.

```{r data, echo=FALSE, message=FALSE}

urlfileupdate="https://raw.githubusercontent.com/sbvsweeney/STATS531-TimeSeries-Midterm/main/co2_mm_mlo.txt"

CO2new<-read.table(url(urlfileupdate))

CO2new <- rename(CO2new,Year="V1", Month="V2", Date="V3", MonthAvg="V4", Deseason_MonthAvg="V5", NumDay="V6", sdDay="V7", Uncert_MonthAvg="V8")

str(CO2new)
CO2n<-count(CO2new)
#head(CO2new)
yearmin<-min(CO2new$Year)
yearmax<-max(CO2new$Year)
monthmean<-summary(CO2new$MonthAvg)
monthsd<-sd(CO2new$MonthAvg)

seasonmean<-summary(CO2new$Deseason_MonthAvg)
seasonsd<-sd(CO2new$Deseason_MonthAvg)


```

### Variables of Interest

**Year / Month (*Year, Month*)**
The Year and Month variables contain the calendar year and month respectively in which the CO~2~ data was collected for the monthly average.  The dataset contains information from `r yearmin` to `r yearmax`, for a total of `r CO2n` observations.

**Decimal Date (*Date*)**
The Date variable is a transformation of the Year/Month variables to a continuous decimal date.  For example, March 1958 is converted to 1958.2027 in the Date variable.  This makes plotting and analysis easier in statistical software.

**Monthly Average (*MonthAvg*)**
Monthly Average is the variable of greatest interest, as it is the level of atmospheric CO~2~ recorded in parts per million (ppm).  Per the NOAA, monthly average values are computed from daily mean values. The monthly averages range from `r monthmean[1]` to `r monthmean[6]`, with a mean of `r round(monthmean[4],digits=2)` $\pm$ `r round(monthsd,digits=2)` and a median of `r monthmean [3]`. 

**Deseasonalized Monthly Average (*Deseason_MonthAvg*)**
NOAA scientists have previously determined a seasonal cycle for CO~2~ levels, which is used to center the monthly averages average seasonal cycle.  In this analysis we will conduct our own review of seasonality/cycle, but this variable has been retained for comparison purposes.  The monthly deseasonalized averages range from `r seasonmean[1]` to `r seasonmean[6]`, with a mean of `r seasonmean[4]` $\pm$ `r seasonsd` and a median of `r seasonmean [3]`. 

### Missing Data

Per the data description provided by the NOAA, missing months have been interpolated and are indicated by negative "Standard Deviation of Number of Days" and "Uncertainty Estimate of Monthly Average" variable values.  Given this imputation, there is no additional treatment needed in this analysis for missing values.

Unknown values are represented as negative numbers throughout the dataset.

\

## Statistical Software and Packages

The analysis described in this report is conducted in R (Vienna, Austria)~7~.  Packages used for the analysis and report are ggplot2, pander, knitr, readr, dplyr, readxl, tidyverse, astsa, aTSA, and forecast.


\

## Exploratory Data Analysis

An exploratory data analysis was conducted to understand the apparent patterns and structure of the data distribution, as well as inform the methods best suited to those characteristics.

### Timeplot

A timeplot of the monthly atmospheric CO~2~ level is shown in Figure 1. A quadratic linear model was fitted using the data and is shown overlying the timeplot in red:

$$ CO_2 = \beta_0 + \beta_1 * Date + \beta_2 * Date^2$$

Figure 1.
```{r time_plot, echo=FALSE, message=FALSE}
qmod<-lm(MonthAvg ~ Date + I(Date^2), data = CO2new)
plot(CO2new$Date,CO2new$MonthAvg,type="l",xlab="date",ylab="CO2", main="CO2 Timeplot with fitted Quadratic Linear Model")
lines(CO2new$Date,qmod$fitted.values, col = "red")
```

An alternative timeplot (Figure 2) provides the standard timeplot, but with a loess fitted line shown in red.  This smoother further demonstrates a trend expected in the atmospheric CO~2~ data.

Figure 2.
```{r time_plot_2, echo=FALSE, message=FALSE}
# Estimating trend by loeess smoothing
co2_loess <- loess(CO2new$MonthAvg ~ CO2new$Date, span=0.5)
plot(CO2new$Date, CO2new$MonthAvg, type='l', main="CO2 Timeplot with fitted Loess Values")
lines(co2_loess$x, co2_loess$fitted, type='l', col='red')
```

The both timeplots reveal several important characteristics about the CO~2~ data: (1) there appears to be a clear trend in the data as CO~2~ levels increase over time but that variance appears to be stable, (2) there is strong evidence of seasonality that will need to be addressed in the methods selected/analysis, and (3) a quadratic linear model is a viable candidate to describe the change in atmospheric CO~2~ levels over the last 60 years.

### Stationarity and Seasonality

Stationarity is an important computational and theoretical assumption in any time series analysis.~8,9~ As noted on the timeplots in Figures 1 and 2, we anticipate the atmospheric CO~2~ levels are not well modeled as mean stationary - there is an apparent upward trend. The timeplots also reveal strong evidence of seasonality in atmospheric CO~2~ levels.  This makes sense intuitively as the use of fossil fuels will vary at different times of the year (with weather changes, increased travel periods, manufacturing cycles, etc.) and the cycles of natural CO~2~ producing system (e.g. volcanoes and photosynthesis).

The auto-correlation (ACF) plot in Figure 3 confirms these observations.  The sample ACF is not consistent with a model having independent error terms, so we reject a null hypothesis that the  residuals of the quadratic linear model are iid.

Figure 3.
```{r qm_ACF, echo=FALSE, message=FALSE}
acf(resid(qmod))
```


### Detrending

To appropriately model the CO~2~ atmospheric levels, given the notable non-stationarity, we must first detrend the data.  This is achieved by considering differenced data, rather than the raw data.~8~ Specifically, we transform data $y^{*}_{1:N}\ to \ z_{2:N}$,

$$Z_n = \Delta y_n^{*} = y_n^{*} - y^{*}_{n-1}$$

```{r difference,echo=FALSE, message=FALSE}
CO2new$diff = rep(NA,nrow(CO2new))

CO2new[1, 'diff'] = 0

for (i in 2:nrow(CO2new)) {
  CO2new[i, 'diff'] = CO2new[i, "MonthAvg"] - CO2new[i-1, "MonthAvg"]
}

```

We perform an augmented Dickey-Fuller test (ADF) to formally test for stationarity. The null hypothesis of ADF test is the time series is not stationary and the alternative hypothesis is stationary. From ADF test, we got a p-value which is smaller than 0.01, thus, we can reject the null hypothesis and accept the alternative hypothesis, the differenced series is stationary and it can be used to conduct an ARMA model.

```{r, message=FALSE, echo = FALSE, result=FALSE}
adf.test(CO2new$diff, output = TRUE)
```

The updated time series plot for the differenced data (Figure 4) shows that the data becomes more stationary. It can also be verified that the slope of a linear regression is almost horizontal, shown in red on Figure 4. 

Figure 4.
```{r updated_timeplot,echo=FALSE, message=FALSE}
plot(diff~Date,data=CO2new,ty="l",xlab="week",ylab="co2 difference",main = "Time series plot of CO2 difference")
abline(lm(diff ~ Date, data = CO2new), col = "red")
```

\

## Frequency Domain Analysis

With strong evidence for seasonality/cycles within the atmospheric CO~2~ data, we begin with Frequency Domain Analysis, which will allow us to quantify those cycles.  Understanding the cycles, again, is useful in implementing policies and initiatives at the right time to be most effective, but also in overall model fitting.

### Spectral Analysis

We can quantify the period of the atmospheric CO~2~ level cycle by investigating its periodogram.  The highest peak on the periodogram corresponds with the frequency we use to calculate the cycle.~10~


$$ Cycle = \frac{1}{frequency} $$
Figure 5 provides a periodogram smoothed with span (10,10).

Figure 5. 
```{r periodogram, echo=FALSE, message=FALSE}
smooth_spectrum = spectrum(CO2new$diff, span = c(10,10), main="Smoothed Periodogram of Growth Rate")
```

```{r spectrum_calc, echo=FALSE, message=FALSE}
max_spec<-smooth_spectrum$freq[which.max(smooth_spectrum$spec)]
period <- 1/smooth_spectrum$freq[which.max(smooth_spectrum$spec)]
```

We can see that the spectrum have a peak at around `r round(max_spec, digits=2)`, which corresponds to period of `r period` months. 

Using the calculated frequency, we can decompose the atmospheric CO~2~ levels into its components - trend, noise, cycle - as shown in Figure 6.

Figure 6
```{r loess_trend, echo=FALSE, message=FALSE}
c_low <- loess(CO2new$MonthAvg ~ CO2new$Date, span=0.4)$fitted

c_high <- CO2new$MonthAvg - loess(CO2new$MonthAvg ~ CO2new$Date, span=0.03)$fitted

c_cycle <- CO2new$MonthAvg - c_low - c_high

trend <- ts(c_low, start = CO2new$Date[1], frequency=12)
noise <- ts(c_high, start = CO2new$Date[1], frequency=12)
cycle <- ts(c_cycle, start = CO2new$Date[1], frequency=12)
co2 <- ts(CO2new$MonthAvg, start = CO2new$Date[1], frequency=12)
plot(ts.union(co2, trend, noise, cycle), main="Decomposition of CO2 concentration as trend + noise + cycles")
```

The decomposition in Figure 6 further supports the trend we found in exploratory data analysis and the cycle calculated from the smoothed periodogram.

```{r spectrum_calc_2, include=FALSE, message=FALSE}
#alternate periodogram
spec_analysis <- spectrum(c_cycle, span=c(3, 5, 3))
most_freq <- spec_analysis$freq[which.max(spec_analysis$spec)]
cycling <- 1 / most_freq
cycling
```

\

## Model Fitting

With appropriately detrended data, we can begin to fit a time series model to the atmospheric CO~2~ level data.  Auto-regressive Moving Averages (ARMA) modeling is common in time series analysis so is investigated, though Seasonal Auto-regressive Moving Averages (SARMA) modeling is anticipated to be most appropriate given the evidence of seasonality observed in exploratory data analysis.

### Fitting ARMA model

The CO~2~ data is fit to an ARMA(p,q) model with the following form~8~:

$$Y_{n}=\mu+\phi_{1}\left(Y_{n-1}-\mu\right)+\cdots+\phi_{p}\left(Y_{n-p}-\mu\right)+\varepsilon_{n}+\psi_{1} \varepsilon_{n-1}+\cdots+\psi_{q} \varepsilon_{n-q},$$
where the error terms are Gaussian white noise, $\varepsilon_n \sim N\left(0, \sigma^{2}\right)$. The parameters $(\phi_{1:p})$ is the coefficients for the autoregressive part of the model, $\psi_{1:p}$ is the coefficients for the moving average part of the model, and $\mu$ is the population mean.

\
We select the best (p,q) using the Akaike’s information criterion (AIC)~11~ which is defined as $$AIC = -2 \times \ell(\theta) + 2D$$

AIC is an approach to minimize prediction error and mitigate overfitting of models.  Using AIC criterion, we select the model with the lowest AIC value, or a competitively low AIC value when less complex models can offer similar results.~11~

Table 1. AIC values for ARMA(p,q) models of the CO~2~ data.
```{r aic_table,echo=FALSE, cache=TRUE, message=FALSE}
aic_mat = matrix(5, 5, 5)
for(i in c(0:5)){
    for(j in c(0:5)){
      aic_mat[i,j] = arima(CO2new$diff, order = c(i, 0, j),optim.control = list(maxit=1000))$aic
    }
}

dimnames(aic_mat) <- list(paste("AR",0:4, sep=""),paste("MA",0:4,sep=""))
# kable(aic_mat,digits=2)
```


Based on the table above, we observe that two competetively low AIC values occur for ARMA(3,3) with an AIC value of `r round(aic_mat[4,4],digits=2)` and ARMA (3,4) with and AIC value of `r round(aic_mat[4,5],digits=2)`. With similar AIC values, we favor the slightly less complex ARMA(3,3) model.

The ARMA(3,3) model coefficients, standard errors, and other characteristic statistics are shown below:
```{r arima, echo=FALSE, message=FALSE}
ar_mod = Arima(CO2new$diff,order=c(3,0,3))
ar_mod
```

We next use the AR and MA polynomial roots ($\phi$ and $\psi$, respectively) to investigate the causality and invertibility of the ARMA(3,3) model.  Causality is an important feature of a model that will be used for forecasting - it requires that future values can be determined only using previously observed values.  Invertibility is also a critical feature of a good ARMA model, as it suggests the stability of that model.  Both are investigated, again, by reviewing the polynomial roots of the model coefficients.  Inverse AR roots outside the unit circle suggest causality and inverse MA roots outside the unit circle suggest invertibility.~12~

Figure 7. AR and MA polynomial roots for ARMA(3,3) model
```{r arplot, echo=FALSE, message=FALSE}
plot(ar_mod)
```

The three dots in the left hand plot correspond to the roots of the AR polynomials $(\phi)$, while the three dots in the right hand plot corresponds to the root of MA polynomials $(\psi)$. The AR roots and MA roots are close to the border of unit circle. It suggests that the fitted model might not be causal and is at the threshold of non-invertibility. This suggests that the ARMA(3,3) model may become unstable and might not be good for forecasting.

Thus, we fit the slightly more complex ARMA(3,4) model which had the lowest AIC value.  But, again we find that the model is potentially unstable by review of the inverse polynomial roots. This suggests the added complexity does not provide notable value toward the causality and invertibility.

```{r aralt1, echo=FALSE, message=FALSE}
ar_alternative1 = Arima(CO2new$diff,order=c(3,0,4))
ar_alternative1
```

Figure 8.
```{r aralt1_plot, echo=FALSE, message=FALSE}
plot(ar_alternative1)
```


Finally, we fit the ARMA (3,2) that is simpler and have the third lowest AIC values among the candidate models to determine if we can achieve a more stable model. We observe that roots of AR polynomials and MA polynomials are further inside the unit circle.


```{r aralt, echo=FALSE, message=FALSE}
ar_alternative = Arima(CO2new$diff,order=c(3,0,2))
ar_alternative
```

Figure 9.
```{r aralt_plot, echo=FALSE, message=FALSE}
plot(ar_alternative)
```

### Wilk’s approximation

As described in the above ARMA model fitting section, we have 3 candidate ARMA models - ARMA(3,2), ARMA(3,3), and ARMA(3,4) - without a clear indication about which model is preferred based on causality or invertibility.  We use Wilk's approximation to determine whether these models are significantly different from one another.  If the models are not significantly differnt, we would choose ARMA(3,2) as it is a simplest model. The null and alternate hypotheses under Wilk's approximation are: 


$$H_{0} \ \ :  \theta \in \Theta_0 = (\phi_{1:3},\psi_{1:2} ,\mu, \sigma^2)$$
$$H_{1} \ \ :  \theta \in \Theta_1 = (\phi_{1:3},\psi_{1:3} ,\mu, \sigma^2)$$

Under the null hypothesis, the Wilk's approximation is the following~11~,


$$\Lambda = (\mathcal{l}^{(1)} - \mathcal{l}^{(0)}) \approx  \frac{1}{2}\chi^2_{D^{(1)}-D^{(0)}}, \ \ \ \ $$

where $D^{(0)}$ and $D^{(1)}$ are the number of estimated parameters for null and alternative hypothesis, $l_{(0)}$ and $l_{(1)}$ are the maximum log-likelihood under null and and alternative hypotheses.

#### ARMA(3,2) vs. ARMA (3,3)

```{r wilks, include=FALSE, message=FALSE}
h0 = arima(CO2new$diff, order = c(3,0,2))
h1 = arima(CO2new$diff, order = c(3,0,3))
log_diff = 2*(h1$loglik - h0$loglik)
log_diff
cutoff = qchisq(0.95, 1)
cutoff
```

The log-likelihood difference is `r round(log_diff,digits=2)`, and the cutoff value is `r round(cutoff,digits=2)`. Since the log-likelihood difference is smaller than the cutoff value, we fail to reject the null hypothesis and conclude that the ARMA(3,2) is not significantly different than ARMA (3,3) and thus we choose ARMA(3,2).

#### ARMA (3,2) vs. ARMA (3,4)

```{r wilks2, include=FALSE, message=FALSE}
h02 = arima(CO2new$diff, order = c(3,0,2))
h12 = arima(CO2new$diff, order = c(3,0,4))
log_diff2 = 2*(h12$loglik - h02$loglik)
log_diff2
cutoff2 = qchisq(0.95, 1)
cutoff2
```

The log-likelihood difference is `r round(log_diff2,digits=2)`, and the cutoff value is `r round(cutoff2,digits=2)`. Again, since the log-likelihood difference is smaller than the cutoff value, we fail to reject the null hypothesis and conclude that the ARMA(3,2) is not significantly different than ARMA (3,4) and thus we choose ARMA(3,2).

### ARMA Model Residual Analysis

After finding the best ARMA model, we check if the model assumptions are met. First, we need to verify if the residuals follow the normal distribution. Based on the QQ plot below, the errors terms are roughly normally distributed.
Figure 10.
```{r qqnorm, echo=FALSE, message=FALSE}
qqnorm(residuals(ar_alternative))
qqline(residuals(ar_alternative),probs = c(0.25,0.75))
```

Figure 11.
```{r wilks_acf, echo=FALSE, message=FALSE}
acf(ar_alternative$resid,lag.max = 30)
```

From the ACF plot of residuals (Figure 11), we can see that there is an seasonal pattern which span across around 12 lags. This makes sense as the use of fossil fuels, industry cycles, and photosynthesis of plants can all reasonably be considered on an annual cycle.


### SARIMA

Throughout the analysis there has been strongly supported evidence for a 12 month cycle in atmospheric CO~2~ levels. SARMA modeling is a special case of ARMA where the AR and MA polynomials are factored into monthly and annual polynomial.  This makes SARMA modeling very effective when there is known seasonality in the data of interest.~9~

### Model Selection

The CO~2~ data is fit to a SARMA(p,q) x (P,Q)~12~ model with the following form~9~:

$$ \phi(B)\Phi(B^{12})(Y_n - \mu) = \psi(B)\Psi(B^{12})\varepsilon_n\,$$
where the error terms are a white noise process and

$$\begin{aligned}
\mu &= E[Y_n] \\
\phi(x) &= 1 - \phi_1x - \dots - \phi_px^p, \\
\psi(x) &= 1 + \psi_1x - \dots + \psi_qx^q, \\
\Phi(x) &= 1 - \Phi_1x - \dots - \Phi_px^p, \\
\Psi(x) &= 1 + \Psi_1x - \dots + \Psi_qx^q.\\
\end{aligned}$$

Like ARMA modeling described in the above section, the parameters $(\phi_{1:p})$ are the coefficients for the autoregressive part of the model, $\psi_{1:p}$ are the coefficients for the moving average part of the model, and $\mu$ is the population mean.  SARMA models further include the seasonal period which is captured by $\Phi$ and $\Psi$.

We select the best parameters for the SARMA model using AIC.  The models with the lowest/competetive AICs are shown in Table 2, along with the parameter settings, log likelihoods, and degrees of freedom.

```{r model_selection, cache=TRUE,include=FALSE, message=FALSE}
# Select the model
i = 0
arma_p = numeric(64)
arma_d = numeric(64)
arma_q = numeric(64)
s_P = numeric(64)
s_D = numeric(64)
s_Q = numeric(64)
aic = numeric(64)
loglik = numeric(64)
for (p in 0:3){
  for (d in 0:1){
    for (q in 0:3){
      for (P in 0:0){
        for (D in 0:0){
          for (Q in 0:1){
            i = i + 1
            m <- sarima(CO2new$MonthAvg, p, d, q, P, D, Q, 12, xreg=CO2new$Date, details = FALSE)
            arma_p[i] = p
            arma_d[i] = d
            arma_q[i] = q
            s_P[i] = P
            s_D[i] = D
            s_Q[i] = Q
            aic[i] = m$AIC
            loglik[i] = m$fit$loglik
          }
        }
      }
    }
  }
}
```

Table 2. AIC table for SARIMA Modeling of CO~2~ data
```{r aic_table_2, echo=FALSE, warning=FALSE, message=FALSE}
dof <- arma_p + arma_q + s_P + s_Q
df <- data.frame(p = arma_p, d = arma_d, q = arma_q, P = s_P, D = s_D, q = s_Q, AIC = aic, LogLikelihood = loglik, dof=dof)
df_ordered<-df[order(df$AIC), ]
kable(head(df_ordered),digits=2)
```

```{r test, include=FALSE}

str(df)
df_ordered$AIC[1]
df_ordered$AIC[2]
df_ordered$AIC[3]

```

From the AIC table we find the most favorable candidates to be:

1. SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`), with an AIC of `r round(df_ordered$AIC[1],digits=2)`
2. SARIMA(`r df_ordered$p[2]`, `r df_ordered$d[2]`, `r df_ordered$q[2]`, `r df_ordered$P[2]`, `r df_ordered$D[2]`, `r df_ordered$q.1[2]`), with an AIC of `r round(df_ordered$AIC[2],digits=2)`
3. SARIMA(`r df_ordered$p[3]`, `r df_ordered$d[3]`, `r df_ordered$q[3]`, `r df_ordered$P[3]`, `r df_ordered$D[3]`, `r df_ordered$q.1[3]`), with an AIC of `r round(df_ordered$AIC[3],digits=2)`
4. SARIMA(`r df_ordered$p[4]`, `r df_ordered$d[4]`, `r df_ordered$q[4]`, `r df_ordered$P[4]`, `r df_ordered$D[4]`, `r df_ordered$q.1[4]`), with an AIC of `r round(df_ordered$AIC[4],digits=2)`

\

#### Candidate Model 1 vs Candidate Model 2
From the candidate models above, we select between model 1 and 2 using the log likelihood test:

$$ \ell^{<1>} - \ell^{<0>} \approx (1/2) \chi^2_{D^{<1>}-D^{<0>},} $$
where $\chi^2_d$ is a chi-squared random variable on d degrees of freedom.

H~0~: SARIMA(`r df_ordered$p[2]`, `r df_ordered$d[2]`, `r df_ordered$q[2]`, `r df_ordered$P[2]`, `r df_ordered$D[2]`, `r df_ordered$q.1[2]`)

H~1~: SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`)

```{r pval_1, include=FALSE, message=FALSE}
# Candidate 1 vs Candidate 2
l0 <- df_ordered$LogLikelihood[2]
l1 <- df_ordered$LogLikelihood[1]
df = abs(sum(df_ordered$p[1],df_ordered$q[1],df_ordered$P[1],df_ordered$Q[1])-sum(df_ordered$p[2],df_ordered$q[2],df_ordered$P[2],df_ordered$Q[2]))
p_val1 <- 1-pchisq(2*(l1-l0), df)
p_val1
```

We find p=`r p_val1`, so we H~0~ reject the null hypothesis and accept SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`).

\

#### Candidate Model 1 vs Candidate Model 3

We now compare the SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) with model candidate 3 - SARIMA(`r df_ordered$p[3]`, `r df_ordered$d[3]`, `r df_ordered$q[3]`, `r df_ordered$P[3]`, `r df_ordered$D[3]`, `r df_ordered$q.1[3]`).

H~0~: SARIMA(`r df_ordered$p[3]`, `r df_ordered$d[3]`, `r df_ordered$q[3]`, `r df_ordered$P[3]`, `r df_ordered$D[3]`, `r df_ordered$q.1[3]`)

H~1~: SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`)

```{r pval_2, include=FALSE, message=FALSE}
# Candidate 1 vs Candidate 3
l0 <- df_ordered$LogLikelihood[3]
l2 <- df_ordered$LogLikelihood[1]
df2 = abs(sum(df_ordered$p[1],df_ordered$q[1],df_ordered$P[1],df_ordered$Q[1])-sum(df_ordered$p[3],df_ordered$q[3],df_ordered$P[3],df_ordered$Q[3]))
p_val2 <- 1-pchisq(2*(l2-l0), df2)
p_val2

```

We find p=`r p_val2`, so we reject the H~0~ hypothesis and accept SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`).

\ 

#### Candidate Model 1 vs Candidate Model 4

We now compare the SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) with model candidate 4 - SARIMA(`r df_ordered$p[4]`, `r df_ordered$d[4]`, `r df_ordered$q[4]`, `r df_ordered$P[4]`, `r df_ordered$D[4]`, `r df_ordered$q.1[4]`) - to aid in making our final selection.

H~0~: SARIMA(`r df_ordered$p[4]`, `r df_ordered$d[4]`, `r df_ordered$q[4]`, `r df_ordered$P[4]`, `r df_ordered$D[4]`, `r df_ordered$q.1[4]`)

H~1~: SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`)

```{r pval_3, include=FALSE, message=FALSE}
# Candidate 1 vs Candidate 4
l0 <- df_ordered$LogLikelihood[4]
l3 <- df_ordered$LogLikelihood[1]
df3 = abs(sum(df_ordered$p[1],df_ordered$q[1],df_ordered$P[1],df_ordered$Q[1])-sum(df_ordered$p[4],df_ordered$q[4],df_ordered$P[4],df_ordered$Q[4]))
p_val3 <- 1-pchisq(2*(l3-l0), df3)
p_val3

```

We find p=`r p_val3`, so we reject the H~0~ hypothesis and accept SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) by AIC.


### Model Summary

The SARIMA model selected by AIC is SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`), however when model diagnostics were performed include polynomial root review, Rolling Test of coefficients, and simulations for confidence intervals.  The SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) was very unstable, so despite the preference from AIC, functionally it was not an acceptable model.

```{r sarima313, include=FALSE, message=FALSE}
m1 <- sarima(CO2new$MonthAvg, 3, 1, 3, 0, 0, 1, 12, xreg=CO2new$Date)
#m1
#m$fit$coef
#m$fit$coef[1]
#m$fit$coef[2]
#m$fit$coef[3]
#m$fit$coef[4]
#m$fit$coef[5]
#m$fit$coef[6]
#m$fit$coef[7]
#m$fit$coef[8]
```

The unit roots (shown below) are within the unit circle which, along with the computations errors found in computing the model, indicate the instability of the SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) model.

SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) AR polynomial roots:
```{r SARMA313_ARpolyroot, echo=FALSE}
# AR polyroot
polyroot(c(1, -1*m1$fit$coef[1], -1*m1$fit$coef[2], -1*m1$fit$coef[3]))
```

SARIMA(`r df_ordered$p[1]`, `r df_ordered$d[1]`, `r df_ordered$q[1]`, `r df_ordered$P[1]`, `r df_ordered$D[1]`, `r df_ordered$q.1[1]`) MA polynomial roots:
```{r SARMA313_MApolyroot, echo=FALSE, message=FALSE}
# MA polyroot
polyroot(c(1, m1$fit$coef[4], m1$fit$coef[5], m1$fit$coef[6]))

```

To address the issues of stability we opt for the less complex model of SARIMA(`r df_ordered$p[4]`, `r df_ordered$d[4]`, `r df_ordered$q[4]`, `r df_ordered$P[4]`, `r df_ordered$D[4]`, `r df_ordered$q.1[4]`), which had a competitive AIC value and allow for much better stability overall.

The SARIMA(`r df_ordered$p[4]`, `r df_ordered$d[4]`, `r df_ordered$q[4]`, `r df_ordered$P[4]`, `r df_ordered$D[4]`, `r df_ordered$q.1[4]`) model coefficients, standard errors, and diagnostics are presented in the Model Diagnostics section.

```{r sarima211_1, echo=FALSE, include=FALSE, message=FALSE}
m211 <- sarima(CO2new$MonthAvg, 2, 1, 1, 0, 0, 1, 12, xreg=CO2new$Date)
m211$ttable
```


### Simulations for Confidence Intervals

A simulation study provides confidence intervals for the AR, MA, and SMA model parameters.  Figure 12 shows density plots for each parameter along with lines for the 95% confidence interval.

```{r simulations, warning = FALSE, error=FALSE, echo=FALSE, message=FALSE}

# Simulation Study for Confidence Interval
nsim <- 500
ar <- c(m211$fit$coef["ar1"], m211$fit$coef["ar2"])
d = 1
ma <- m211$fit$coef["ma1"]
sar <- 0
D = 0
sma <- m211$fit$coef["sma1"]
theta <- matrix(NA, nrow=nsim, ncol=4, dimnames=list(NULL, c("AR1", "AR2", "MA1", "SMA1")))
for (i in 1:nsim){
  simu <- sarima.sim(ar=ar, d=d, ma=ma, sar=sar, D=D, sma=sma, S=12, n=length(CO2new$MonthAvg))
  m <- sarima(simu, 2, 1, 1, 0, 0, 1, 12, details=FALSE)
  theta[i, 1] = m$fit$coef["ar1"]
  theta[i, 2] = m$fit$coef["ar2"]
  theta[i, 3] = m$fit$coef["ma1"]
  theta[i, 4] = m$fit$coef["sma1"]
}

```

Figure 12.
```{r ci_plots, echo=FALSE, message=FALSE}
# Plot the confidence interval
par(mfrow=c(2, 2))
plot(density(theta[, 'AR1']), type='l', main="AR1 Bootstrapping")
ar1_low = m211$fit$coef["ar1"] - 1.96 * m211$ttable['ar1', 'SE']
ar1_high = m211$fit$coef["ar1"] + 1.96 * m211$ttable['ar1', 'SE']
abline(v=ar1_low, col='red')
abline(v=ar1_high, col='red')

plot(density(theta[, 'AR2']), type='l', main="AR2 Bootstrapping")
ar2_low = m211$fit$coef["ar2"] - 1.96 * m211$ttable['ar2', 'SE']
ar2_high = m211$fit$coef["ar2"] + 1.96 * m211$ttable['ar2', 'SE']
abline(v=ar2_low, col='red')
abline(v=ar2_high, col='red')

plot(density(theta[, 'MA1']), type='l', main="MA1 Bootstrapping")
ma1_low = m211$fit$coef["ma1"] - 1.96 * m211$ttable['ma1', 'SE']
ma1_high = m211$fit$coef["ma1"] + 1.96 * m211$ttable['ma1', 'SE']
abline(v=ma1_low, col='red')
abline(v=ma1_high, col='red')

plot(density(theta[, 'SMA1']), type='l', main="SMA1 Bootstrapping")
sma1_low = m211$fit$coef["sma1"] - 1.96 * m211$ttable['sma1', 'SE']
sma1_high = m211$fit$coef["sma1"] + 1.96 * m211$ttable['sma1', 'SE']
abline(v=sma1_low, col='red')
abline(v=sma1_high, col='red')
```

From the bootstrap, we can see that the fisher confidence interval for AR1, AR2 and SMA1 is more reliable, but for MA1, it's not that reliable because the root of MA1 is close to the edge of unit circle.

### Model Diagnostics

#### Residuals
The model residuals and q-q plot are shown in the figure below:

```{r sarima211, echo=FALSE, message=FALSE, result=FALSE}
sarima(CO2new$MonthAvg, 2, 1, 1, 0, 0, 1, 12, xreg=CO2new$Date)
```

The residuals still show some autocorrelation that we should be aware of as we interpret the final results from the SARIMA model.  This does, however, show improvement from the ARMA model.

The Q-Q plot supports the assumption that the residuals are generally normally distributed.

\

#### Polynomial Root Review for Causality and Invertibility

As described in the SAIRMA model fitting section, we review the AR and MA polynomial roots to review causality and invertibility of the model.

The AR roots are:
```{r AR_root_SARMA, echo=FALSE, message=FALSE}
polyroot(c(1, -1*m211$fit$coef[1], -1*m211$fit$coef[2]))
```

This are within the unit circle, which suggests our model is not necessarily causal and will have limitations where it comes to prediction.  However, this again represents an improvement from the ARMA model.
\

The MA root is:
```{r MA_root_SARMA, echo=FALSE, message=FALSE}
polyroot(c(1, m211$fit$coef[3]))
```

The MA root is outside the unit circle, which means our model is invertible and will remain more stable.  This is preferable to both the ARMA model and the initial AIC-selected SARIMA(3,1,3,0,0,1) which had MA roots within the unit circle.

\

#### Rolling Test for coefficients

Cross-validation is a valuable tool in model fitting, but is not possible within time dependent structure, as in time series.  Instead, we conduct a rolling test of coefficients.  A good model should have relatively stable coefficients over rolling time periods.

Figure 13 shows the model coefficients on rolling time bases. The plots show generally stable coefficients, though the MA component fluctuates notably.  In future work, additional attention on the MA component of the model may provide more opportunity to fine tune for slight improvements.


```{r rolling, warning = FALSE, error=FALSE, echo=FALSE, message=FALSE}
# A good model should have the relatively stable coefficients
nwindow <- 500
rolling_theta <- matrix(NA, nrow=nwindow, ncol=4, dimnames=list(NULL, c("AR1", "AR2", "MA1", "SMA1")))
for (n in 1:nwindow){
  data <- CO2new[(1+n):(200+n),]
  m <- sarima(data$MonthAvg, 2, 1, 1, 0, 0, 1, 12, xreg=data$Date, details=FALSE)
  rolling_theta[n, "AR1"] = m$fit$coef["ar1"]
  rolling_theta[n, "AR2"] = m$fit$coef["ar2"]
  rolling_theta[n, "MA1"] = m$fit$coef["ma1"]
  rolling_theta[n, "SMA1"] = m$fit$coef["sma1"]
}
```

Figure 13.
```{r rolling_plots, echo=FALSE, message=FALSE}
# Plot the rolling coefficients
par(mfrow=c(2, 2))
plot(rolling_theta[, "AR1"], type='l', ylim=c(1.43, 1.52), main="rolling AR1", xlab="window", ylab="magnitude")
plot(rolling_theta[, "AR2"], type='l', ylim=c(-0.8, -0.75), main="rolling AR2", xlab="window", ylab="magnitude")
plot(rolling_theta[, "MA1"], type='l', ylim=c(-1, -0.94), main="rolling MA1", xlab="window", ylab="magnitude")
plot(rolling_theta[, "SMA1"], type='l', ylim=c(0.35, 0.55), main="rolling SMA1", xlab="window", ylab="magnitude")
```


## Prediction

Finally, we perform a final test of our model accuracy by applying the model for prediction.  We apply a rolling prediction patter and use a 100 time point window to attempt to predict beyond the window.  Figure 14 shows the predictive performance compared with the actual data from the atmospheric CO~2~ data.  The performance looks quite good, but additional testing and comfirming future forecasts can increase the confidence in this model.

```{r prediction, warning = FALSE, error=FALSE, echo=FALSE, message=FALSE}
# Here, we adhere a rolling prediction pattern
nwindow <- 500
pred <- numeric(nwindow)
for (n in 1:nwindow){
  try({
    data = CO2new[(1+n):(200+n),]
    new_xreg <- CO2new[201 + n, ]
    prediction <- sarima.for(data$MonthAvg, n.ahead=1, 2, 1, 1, 0, 0, 1, 12, plot=FALSE, xreg=data$Date, newxreg = new_xreg$Date)
    pred[n] <- prediction$pred
  })
}
```
Figure 14.
```{r pred_plot, echo=FALSE, message=FALSE}
new <- CO2new[202:701, ]
plot(new$Date[pred != 0], subset(new$MonthAvg, pred != 0), type='l', main="CO2 prediction", xlab="Date", ylab="Concentration")
lines(new$Date[pred != 0], subset(pred, pred != 0), col='red')
```

## Conclusion

Through this investigation we sought to corroborate the reported trend of atmospheric CO~2~ level, understand the underlying structure such as seasonality, and use modeling in both the time and frequency domains to quantify the associated trends and cycles of atmospheric CO~2~ levels. 

We were, indeed, able to confirm the reported trend and 12 month cycle in CO~2~ atmospheric levels.  CO~2~ levels in the atmosphere are rising which is, in turn, contributing to the rising global temperature.  This pattern indicates an urgent need to implement policies that will begin to reverse or mitigate CO~2~ released by human activity.


## References

1. NASA. (2021, November 16). Carbon dioxide concentration. NASA. Retrieved February 21, 2022, from https://climate.nasa.gov/vital-signs/carbon-dioxide/ 
2. NASA. (2020, February 5). Graphic: The relentless rise of carbon dioxide – climate change: Vital signs of the planet. NASA. Retrieved February 21, 2022, from https://climate.nasa.gov/climate_resources/24/graphic-the-relentless-rise-of-carbon-dioxide/ 
3. Temperature change and carbon dioxide ... - NCEI offers acces. (n.d.). Retrieved February 21, 2022, from https://www.ncei.noaa.gov/sites/default/files/2021-11/8%20-%20Temperature%20Change%20and%20Carbon%20Dioxide%20Change%20-%20FINAL%20OCT%202021.pdf 
4. NASA. (2022, January 19). Global surface temperature. NASA. Retrieved February 21, 2022, from https://climate.nasa.gov/vital-signs/global-temperature/ 
5. US Department of Commerce, N. O. A. A. (2005, October 1). Global Monitoring Laboratory - Carbon Cycle Greenhouse Gases. GML. Retrieved February 21, 2022, from https://gml.noaa.gov/ccgg/trends/trends_log.html  [ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt]
6. US Department of Commerce, N. O. A. A. (2005, October 1). ESRL Global Monitoring Laboratory - FTP navigator. GML. Retrieved February 21, 2022, from https://gml.noaa.gov/dv/data/index.php?category=Greenhouse%2BGases¶meter_name=Carbon%2BDioxide&amp;site=MLO 
7. R Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
8. Ionides, E. L. (n.d.). Modeling and analysis of time series Datastats/datasci 531, winter 2022Chapter 3: Stationarity, White Noise, and some basic time series models. Modeling and Analysis of Time Series Data STATS/DATASCI 531, Winter 2022 Chapter 3: Stationarity, white noise, and some basic time series models. Retrieved February 21, 2022, from https://ionides.github.io/531w22/03/index.html 
9. Ionides, E. L. (n.d.). Modeling and analysis of time series Datastats/datasci 531, winter 2022 Chapter 6: Extending the ARMA model: Seasonality, integration and trend. Modeling and Analysis of Time Series Data STATS/DATASCI 531, Winter 2022 Chapter 6: Extending thh ARMA model: Seasonality, integration and trend. Retrieved February 21, 2022, from https://ionides.github.io/531w22/06/slides.pdf
10. Ionides, E. L. (n.d.). Modeling and analysis of time series Datastats/datasci 531, Winter 2022Chapter 7: Introduction to time series analysis in the frequency domain. Modeling and Analysis of Time Series Data STATS/DATASCI 531, Winter 2022 Chapter 7: Introduction to time series analysis in the frequency domain. Retrieved February 21, 2022, from https://ionides.github.io/531w22/07/index.html 
11. 2mm modeling and analysis of time ... - ionides.github.io. (n.d.). Retrieved February 21, 2022, from https://ionides.github.io/531w22/05/slides.pdf 
12. Ionides, E. L. (n.d.). Modeling and analysis of time series Datastats/datasci 531, winter 2022Chapter 4: Linear Time Series models and the algebra of Arma Models. Modeling and Analysis of Time Series Data STATS/DATASCI 531, Winter 2022 Chapter 4: Linear time series models and the algebra of ARMA models. Retrieved February 21, 2022, from https://ionides.github.io/531w22/04/index.html 
